{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "python_random.seed(42)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "oleh_dataset = pd.read_csv('dataset.csv')\n",
    "oleh_dataset = oleh_dataset[oleh_dataset['label'] != 'unknown']\n",
    "\n",
    "\n",
    "with open('show-validation.txt', 'r') as f:\n",
    "    show_sents = f.readlines()\n",
    "with open('tell-validation.txt', 'r') as f:\n",
    "    tell_sents = f.readlines()\n",
    "scraped_dataset = pd.DataFrame({'sentence': show_sents + tell_sents,\n",
    "                                'label': ['show'] * len(show_sents) + ['tell'] * len(tell_sents)})\n",
    "scraped_dataset['sentence'] = scraped_dataset['sentence'].str.strip()\n",
    "scraped_dataset['label'].value_counts()\n",
    "\n",
    "\n",
    "with open('katia-show.txt', 'r') as f:\n",
    "    show_sents = f.readlines()\n",
    "with open('katia-tell.txt', 'r') as f:\n",
    "    tell_sents = f.readlines()\n",
    "katia_dataset = pd.DataFrame({'sentence': show_sents + tell_sents,\n",
    "                              'label': ['show'] * len(show_sents) + ['tell'] * len(tell_sents)})\n",
    "katia_dataset['sentence'] = katia_dataset['sentence'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    cleaned = re.sub(r'([^\\s\\w]|_)+', '', sentence)\n",
    "    return cleaned.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "oleh_tokenized = [simple_tokenize(sentence) for sentence in oleh_dataset['sentence']]\n",
    "scraped_tokenized = [simple_tokenize(sentence) for sentence in scraped_dataset['sentence']]\n",
    "katia_tokenized = [simple_tokenize(sentence) for sentence in katia_dataset['sentence']]\n",
    "all_tokenized = oleh_tokenized + scraped_tokenized + katia_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/oleh.palianytsia/Downloads/wiki-news-300d-1M.vec'\n",
    "\n",
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "    return data\n",
    "\n",
    "vocab_and_vectors = load_vectors(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = 5000\n",
    "tokenizer = Tokenizer(num_words = features)\n",
    "tokenizer.fit_on_texts(all_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words that the tokenizer knows\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# put the tokens in a matrix\n",
    "X = tokenizer.texts_to_sequences(oleh_tokenized)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_size = X.shape[1]\n",
    "sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show</th>\n",
       "      <th>tell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1327 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      show  tell\n",
       "0        0     1\n",
       "1        0     1\n",
       "2        0     1\n",
       "3        0     1\n",
       "4        0     1\n",
       "...    ...   ...\n",
       "1322     1     0\n",
       "1323     1     0\n",
       "1324     0     1\n",
       "1325     0     1\n",
       "1326     1     0\n",
       "\n",
       "[1327 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(oleh_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = vocab_and_vectors.get(word)\n",
    "    # words that cannot be found will be set to 0\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 174, 300)          1842300   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 2,564,102\n",
      "Trainable params: 721,802\n",
      "Non-trainable params: 1,842,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.layers import Dropout\n",
    "# init model\n",
    "model = Sequential()\n",
    "# emmbed word vectors\n",
    "model.add(Embedding(len(word_index)+1,300,input_length=X.shape[1],weights=[embedding_matrix],trainable=False))\n",
    "\n",
    "model.add(LSTM(300,return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1],activation=\"softmax\"))\n",
    "# output model skeleton\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/332 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3296596544334687, 0.7831325531005859]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/332 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5158001091106829, 0.7891566157341003]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, sentences, proba=True):\n",
    "    tokenized = [simple_tokenize(s) for s in sentences]\n",
    "    sequences = tokenizer.texts_to_sequences(tokenized)\n",
    "    sequences = pad_sequences(sequences, maxlen=sequence_size)\n",
    "    if proba:\n",
    "        return model.predict(sequences)\n",
    "    else: \n",
    "        return ['tell' if tell_proba > show_proba else 'show' for show_proba, tell_proba in model.predict(sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8806521 , 0.11934789]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict(model, ['Tears are running down Seva\\' cheeks.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(model):\n",
    "    return [tok.text for tok in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "oleh_docs = oleh_dataset['sentence'].apply(nlp)\n",
    "scraped_docs = scraped_dataset['sentence'].apply(nlp)\n",
    "katia_docs = katia_dataset['sentence'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_oleh_train, X_oleh_test, y_oleh_train, y_oleh_test = train_test_split(oleh_docs, oleh_dataset['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def ds_func(f):\n",
    "    return lambda X: [f(x) for x in X]\n",
    "\n",
    "def combine_extractors(funcs):\n",
    "    def combined(x):\n",
    "        feats = {}\n",
    "        for e in funcs:\n",
    "            feats.update(e(x))\n",
    "        return feats\n",
    "    return combined\n",
    "\n",
    "def make_rfc_classifier(*feature_extractors):\n",
    "    classifier = Pipeline([('extractor', FunctionTransformer()),\n",
    "                           ('dict_vect', DictVectorizer()),\n",
    "                           ('rfc', RandomForestClassifier(random_state=42))])\n",
    "    params = {'extractor__func': ds_func(combine_extractors(feature_extractors))}\n",
    "    classifier.set_params(**params)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def make_lrc_classifier(*feature_extractors):\n",
    "    classifier = Pipeline([('extractor', FunctionTransformer()),\n",
    "                           ('dict_vect', DictVectorizer()),\n",
    "                           ('lrc', LogisticRegression())])\n",
    "        \n",
    "    params = {'lrc__random_state': 42,\n",
    "              'lrc__solver': 'sag',\n",
    "              'lrc__multi_class': 'multinomial',\n",
    "              'lrc__max_iter': 5000,\n",
    "              'extractor__func': ds_func(combine_extractors(feature_extractors))}\n",
    "    classifier.set_params(**params)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_main_token(doc):\n",
    "    return [tok for tok in doc if tok.dep_ == 'ROOT'][0]\n",
    "\n",
    "def extract_subj_verb(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        feats['main-word'] = main.text\n",
    "        feats['main-pos'] = main.pos_\n",
    "        feats['main-lemma'] = main.lemma_\n",
    "        \n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            feats['subj-word'] = subj.text\n",
    "            feats['subj-pos'] = subj.pos_\n",
    "            feats['subj-lemma'] = subj.lemma_\n",
    "            \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mom, My, None, None], [cats, very, much, None])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ctx(x, size, check_important=False):\n",
    "    lefts = x.doc[:x.i]\n",
    "    rights = x.doc[x.i+1:]\n",
    "\n",
    "    left_ctx = [x for x in lefts if not check_important or is_important(x)][-size:]\n",
    "    if len(left_ctx) < size:\n",
    "        left_ctx = ([None] * (size - len(left_ctx))) + left_ctx\n",
    "    \n",
    "    right_ctx = [x for x in rights if not check_important or is_important(x)][:size]\n",
    "    if len(right_ctx) < size:\n",
    "        right_ctx = right_ctx + ([None] * (size - len(right_ctx)))\n",
    "\n",
    "    return list(reversed(left_ctx)), right_ctx\n",
    "\n",
    "ctx(nlp('My mom likes cats very much')[2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_verb_ctx(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        left_ctx, right_ctx = ctx(main, 3)\n",
    "\n",
    "#         feats['main-4-word'] = left_ctx[3].text if left_ctx[3] else '<<<none>>>'\n",
    "#         feats['main-4-pos'] = left_ctx[3].pos_ if left_ctx[3] else '<<<none>>>'\n",
    "#         feats['main-4-lemma'] = left_ctx[3].lemma_ if left_ctx[3] else '<<<none>>>'\n",
    "        feats['main-3-word'] = left_ctx[2].lower_ if left_ctx[2] else '<<<none>>>'\n",
    "        feats['main-2-word'] = left_ctx[1].lower_ if left_ctx[1] else '<<<none>>>'\n",
    "        feats['main-1-word'] = left_ctx[0].lower_ if left_ctx[0] else '<<<none>>>'\n",
    "        feats['main+1-word'] = right_ctx[0].lower_ if right_ctx[0] else '<<<none>>>'\n",
    "        feats['main+2-word'] = right_ctx[1].lower_ if right_ctx[1] else '<<<none>>>'\n",
    "        feats['main+3-word'] = right_ctx[2].lower_ if right_ctx[2] else '<<<none>>>'\n",
    "        \n",
    "        \n",
    "        feats['main-3-pos'] = left_ctx[2].pos_ if left_ctx[2] else '<<<none>>>'\n",
    "        feats['main-2-pos'] = left_ctx[1].pos_ if left_ctx[1] else '<<<none>>>'\n",
    "        feats['main-1-pos'] = left_ctx[0].pos_ if left_ctx[0] else '<<<none>>>'\n",
    "        feats['main+1-pos'] = right_ctx[0].pos_ if right_ctx[0] else '<<<none>>>'\n",
    "        feats['main+2-pos'] = right_ctx[1].pos_ if right_ctx[1] else '<<<none>>>'\n",
    "        feats['main+3-pos'] = right_ctx[2].pos_ if right_ctx[2] else '<<<none>>>'\n",
    "        \n",
    "        feats['main-3-lemma'] = left_ctx[2].lemma_ if left_ctx[2] else '<<<none>>>'\n",
    "        feats['main-2-lemma'] = left_ctx[1].lemma_ if left_ctx[1] else '<<<none>>>'\n",
    "        feats['main-1-lemma'] = left_ctx[0].lemma_ if left_ctx[0] else '<<<none>>>'\n",
    "        feats['main+1-lemma'] = right_ctx[0].lemma_ if right_ctx[0] else '<<<none>>>'\n",
    "        feats['main+2-lemma'] = right_ctx[1].lemma_ if right_ctx[1] else '<<<none>>>'\n",
    "        feats['main+3-lemma'] = right_ctx[2].lemma_ if right_ctx[2] else '<<<none>>>'\n",
    "\n",
    "#         feats['main-3-is-emotion'] = left_ctx[2].lower_ in emotions if left_ctx[2] else False\n",
    "#         feats['main-2-is-emotion'] = left_ctx[1].lower_ in emotions if left_ctx[1] else False\n",
    "#         feats['main-1-is-emotion'] = left_ctx[0].lower_ in emotions if left_ctx[0] else False\n",
    "#         feats['main+1-is-emotion'] = right_ctx[0].lower_ in emotions if right_ctx[0] else False\n",
    "#         feats['main+2-is-emotion'] = right_ctx[1].lower_ in emotions if right_ctx[1] else False\n",
    "#         feats['main+3-is-emotion'] = right_ctx[2].lower_ in emotions if right_ctx[2] else False\n",
    "\n",
    "#         feats['main-3-is-emotion'] = left_ctx[2].lower_ in emotions if left_ctx[2] else False\n",
    "#         feats['main-2-is-emotion'] = left_ctx[1].lower_ in emotions if left_ctx[1] else False\n",
    "#         feats['main-1-is-emotion'] = left_ctx[0].lower_ in emotions if left_ctx[0] else False\n",
    "#         feats['main+1-is-emotion'] = right_ctx[0].lower_ in emotions if right_ctx[0] else False\n",
    "#         feats['main+2-is-emotion'] = right_ctx[1].lower_ in emotions if right_ctx[1] else False\n",
    "#         feats['main+3-is-emotion'] = right_ctx[2].lower_ in emotions if right_ctx[2] else False\n",
    "\n",
    "#         feats['main-3-n-synonyms'] = len(pos_synsets(left_ctx[2].lemma_, left_ctx[2].pos_)) if left_ctx[2] else 0\n",
    "#         feats['main-2-n-synonyms'] = len(pos_synsets(left_ctx[1].lemma_, left_ctx[1].pos_)) if left_ctx[1] else 0\n",
    "#         feats['main-1-n-synonyms'] = len(pos_synsets(left_ctx[0].lemma_, left_ctx[0].pos_)) if left_ctx[0] else 0\n",
    "#         feats['main+1-n-synonyms'] = len(pos_synsets(right_ctx[0].lemma_, right_ctx[0].pos_)) if right_ctx[0] else False\n",
    "#         feats['main+2-n-synonyms'] = len(pos_synsets(right_ctx[1].lemma_, right_ctx[1].pos_)) if right_ctx[1] else False\n",
    "#         feats['main+3-n-synonyms'] = len(pos_synsets(right_ctx[2].lemma_, right_ctx[2].pos_)) if right_ctx[2] else False\n",
    "\n",
    "#         feats['main-3-abstract'] = left_ctx[2].lower_ in abstracts if left_ctx[2] else False\n",
    "#         feats['main-2-abstract'] = left_ctx[1].lower_ in abstracts if left_ctx[1] else False\n",
    "#         feats['main-1-abstract'] = left_ctx[0].lower_ in abstracts if left_ctx[0] else False\n",
    "#         feats['main+1-abstract'] = right_ctx[0].lower_ in abstracts if right_ctx[0] else False\n",
    "#         feats['main+2-abstract'] = right_ctx[1].lower_ in abstracts if right_ctx[1] else False\n",
    "#         feats['main+3-abstract'] = right_ctx[2].lower_ in abstracts if right_ctx[2] else False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         feats['main-3-freq-dicr'] = unigram_freq_discr(left_ctx[2].text) if left_ctx[2] else 0\n",
    "#         feats['main-2-freq-dicr'] = unigram_freq_discr(left_ctx[1].text) if left_ctx[1] else 0\n",
    "#         feats['main-1-freq-dicr'] = unigram_freq_discr(left_ctx[0].text) if left_ctx[0] else 0\n",
    "#         feats['main+1-freq-dicr'] = unigram_freq_discr(right_ctx[0].text) if right_ctx[0] else 0\n",
    "#         feats['main+2-freq-dicr'] = unigram_freq_discr(right_ctx[1].text) if right_ctx[1] else 0\n",
    "#         feats['main+3-freq-dicr'] = unigram_freq_discr(right_ctx[2].text) if right_ctx[2] else 0\n",
    "\n",
    "#         feats['main+4-word'] = right_ctx[3].text if right_ctx[3] else '<<<none>>>'\n",
    "#         feats['main+4-pos'] = right_ctx[3].pos_ if right_ctx[3] else '<<<none>>>'\n",
    "#         feats['main+4-lemma'] = right_ctx[3].lemma_ if right_ctx[3] else '<<<none>>>'\n",
    "        \n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            left_ctx, right_ctx = ctx(subj, 3)\n",
    "#             feats['subj-4-word'] = left_ctx[3].text if left_ctx[3] else '<<<none>>>'\n",
    "#             feats['subj-4-pos'] = left_ctx[3].pos_ if left_ctx[3] else '<<<none>>>'\n",
    "#             feats['subj-4-lemma'] = left_ctx[3].lemma_ if left_ctx[3] else '<<<none>>>'\n",
    "            \n",
    "            feats['subj-3-word'] = left_ctx[2].lower_ if left_ctx[2] else '<<<none>>>'\n",
    "            feats['subj-2-word'] = left_ctx[1].lower_ if left_ctx[1] else '<<<none>>>'\n",
    "            feats['subj-1-word'] = left_ctx[0].lower_ if left_ctx[0] else '<<<none>>>'\n",
    "            feats['subj+1-word'] = right_ctx[0].lower_ if right_ctx[0] else '<<<none>>>'\n",
    "            feats['subj+2-word'] = right_ctx[1].lower_ if right_ctx[1] else '<<<none>>>'\n",
    "            feats['subj+3-word'] = right_ctx[2].lower_ if right_ctx[2] else '<<<none>>>'\n",
    "            \n",
    "            feats['subj-3-pos'] = left_ctx[2].pos_ if left_ctx[2] else '<<<none>>>'\n",
    "            feats['subj-2-pos'] = left_ctx[1].pos_ if left_ctx[1] else '<<<none>>>'\n",
    "            feats['subj-1-pos'] = left_ctx[0].pos_ if left_ctx[0] else '<<<none>>>'\n",
    "            feats['subj+1-pos'] = right_ctx[0].pos_ if right_ctx[0] else '<<<none>>>'\n",
    "            feats['subj+2-pos'] = right_ctx[1].pos_ if right_ctx[1] else '<<<none>>>'\n",
    "            feats['subj+3-pos'] = right_ctx[2].pos_ if right_ctx[2] else '<<<none>>>'\n",
    "            \n",
    "            feats['subj-3-lemma'] = left_ctx[2].lemma_ if left_ctx[2] else '<<<none>>>'\n",
    "            feats['subj-2-lemma'] = left_ctx[1].lemma_ if left_ctx[1] else '<<<none>>>'\n",
    "            feats['subj-1-lemma'] = left_ctx[0].lemma_ if left_ctx[0] else '<<<none>>>'\n",
    "            feats['subj+1-lemma'] = right_ctx[0].lemma_ if right_ctx[0] else '<<<none>>>'\n",
    "            feats['subj+2-lemma'] = right_ctx[1].lemma_ if right_ctx[1] else '<<<none>>>'\n",
    "            feats['subj+3-lemma'] = right_ctx[2].lemma_ if right_ctx[2] else '<<<none>>>'\n",
    "\n",
    "#             feats['subj-3-is-emotion'] = left_ctx[2].lower_ in emotions if left_ctx[2] else False\n",
    "#             feats['subj-2-is-emotion'] = left_ctx[1].lower_ in emotions if left_ctx[1] else False\n",
    "#             feats['subj-1-is-emotion'] = left_ctx[0].lower_ in emotions if left_ctx[0] else False\n",
    "#             feats['subj+1-is-emotion'] = right_ctx[0].lower_ in emotions if right_ctx[0] else False\n",
    "#             feats['subj+2-is-emotion'] = right_ctx[1].lower_ in emotions if right_ctx[1] else False\n",
    "#             feats['subj+3-is-emotion'] = right_ctx[2].lower_ in emotions if right_ctx[2] else False\n",
    "\n",
    "#             feats['subj-3-abstract'] = left_ctx[2].lower_ in abstracts if left_ctx[2] else False\n",
    "#             feats['subj-2-abstract'] = left_ctx[1].lower_ in abstracts if left_ctx[1] else False\n",
    "#             feats['subj-1-abstract'] = left_ctx[0].lower_ in abstracts if left_ctx[0] else False\n",
    "#             feats['subj+1-abstract'] = right_ctx[0].lower_ in abstracts if right_ctx[0] else False\n",
    "#             feats['subj+2-abstract'] = right_ctx[1].lower_ in abstracts if right_ctx[1] else False\n",
    "#             feats['subj+3-abstract'] = right_ctx[2].lower_ in abstracts if right_ctx[2] else False\n",
    "            \n",
    "#             feats['subj-3-n-synonyms'] = len(pos_synsets(left_ctx[2].lemma_, left_ctx[2].pos_)) if left_ctx[2] else 0\n",
    "#             feats['subj-2-n-synonyms'] = len(pos_synsets(left_ctx[1].lemma_, left_ctx[1].pos_)) if left_ctx[1] else 0\n",
    "#             feats['subj-1-n-synonyms'] = len(pos_synsets(left_ctx[0].lemma_, left_ctx[0].pos_)) if left_ctx[0] else 0\n",
    "#             feats['subj+1-n-synonyms'] = len(pos_synsets(right_ctx[0].lemma_, right_ctx[0].pos_)) if right_ctx[0] else False\n",
    "#             feats['subj+2-n-synonyms'] = len(pos_synsets(right_ctx[1].lemma_, right_ctx[1].pos_)) if right_ctx[1] else False\n",
    "#             feats['subj+3-n-synonyms'] = len(pos_synsets(right_ctx[2].lemma_, right_ctx[2].pos_)) if right_ctx[2] else False\n",
    "\n",
    "#             feats['subj-3-freq-dicr'] = unigram_freq_discr(left_ctx[2].text) if left_ctx[2] else 0\n",
    "#             feats['subj-2-freq-dicr'] = unigram_freq_discr(left_ctx[1].text) if left_ctx[1] else 0\n",
    "#             feats['subj+1-freq-dicr'] = unigram_freq_discr(right_ctx[0].text) if right_ctx[0] else 0\n",
    "#             feats['subj+2-freq-dicr'] = unigram_freq_discr(right_ctx[1].text) if right_ctx[1] else 0\n",
    "#             feats['subj+3-freq-dicr'] = unigram_freq_discr(right_ctx[2].text) if right_ctx[2] else 0\n",
    "\n",
    "#             feats['subj+4-word'] = right_ctx[3].text if right_ctx[3] else '<<<none>>>'\n",
    "#             feats['subj+4-pos'] = right_ctx[3].pos_ if right_ctx[3] else '<<<none>>>'\n",
    "#             feats['subj+4-lemma'] = right_ctx[3].lemma_ if right_ctx[3] else '<<<none>>>'\n",
    "\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vector_to_feats(prefix, vector):\n",
    "    feats = {}\n",
    "    \n",
    "    for i, x in enumerate(vector):\n",
    "        feats[prefix + str(i)] = x\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def avg_vector(vectors):\n",
    "    vect = np.zeros(300)\n",
    "    \n",
    "    for v in vectors:\n",
    "        vect += v\n",
    "    return vect / len(vectors) if len(vectors) > 0 else vect\n",
    "\n",
    "def is_important(x):\n",
    "    return not x.is_stop and not x.pos_ == 'PROPN' and x.ent_iob_ == 'O'\n",
    "\n",
    "def extract_vector(doc):        \n",
    "    return vector_to_feats('sent_vect', avg_vector([x.vector for x in doc if is_important(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_verb_vector(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        feats.update(vector_to_feats('main_vect', main.vector))\n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            feats.update(vector_to_feats('main_subj_vect', subj.vector))\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.76      0.55      0.63       110\n",
      "        tell       0.80      0.91      0.85       222\n",
      "\n",
      "    accuracy                           0.79       332\n",
      "   macro avg       0.78      0.73      0.74       332\n",
      "weighted avg       0.79      0.79      0.78       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_lrc_classifier(extract_vector, extract_subj_verb)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "print(classification_report(y_oleh_test, clf.predict(X_oleh_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrc_predict(clf, sentences, proba=True):\n",
    "    docs = [nlp(s) for s in sentences]\n",
    "    if proba:\n",
    "        return clf.predict_proba(docs)\n",
    "    else:\n",
    "        return clf.predict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59644908, 0.40355092]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc_predict(clf, ['I am sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04585582, 0.95414424]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict(model, ['I am sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(model, clf, sentences):\n",
    "    model_probas = model_predict(model, sentences)\n",
    "    lrc_probas = lrc_predict(clf, sentences)\n",
    "    \n",
    "    return ['tell' if tell_proba > show_proba else 'show' for show_proba, tell_proba in model_probas + lrc_probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.77      0.56      0.65       110\n",
      "        tell       0.81      0.91      0.86       222\n",
      "\n",
      "    accuracy                           0.80       332\n",
      "   macro avg       0.79      0.74      0.75       332\n",
      "weighted avg       0.79      0.80      0.79       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_oleh_test, ensemble_predict(model, clf, [x.text for x in X_oleh_test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.67      0.52      0.58       165\n",
      "        tell       0.80      0.89      0.84       366\n",
      "\n",
      "    accuracy                           0.77       531\n",
      "   macro avg       0.74      0.70      0.71       531\n",
      "weighted avg       0.76      0.77      0.76       531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pd.concat([y_oleh_test, \n",
    "                                       katia_dataset['label'], \n",
    "                                       scraped_dataset['label'],\n",
    "                                      ]), \n",
    "                            model_predict(model, pd.concat([X_oleh_test.apply(lambda x: x.text), \n",
    "                                                            katia_dataset['sentence'], \n",
    "                                                            scraped_dataset['sentence']]),\n",
    "                                         proba=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.63      0.43      0.51       165\n",
      "        tell       0.78      0.89      0.83       366\n",
      "\n",
      "    accuracy                           0.74       531\n",
      "   macro avg       0.70      0.66      0.67       531\n",
      "weighted avg       0.73      0.74      0.73       531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pd.concat([y_oleh_test,\n",
    "                                       katia_dataset['label'], \n",
    "                                       scraped_dataset['label'],\n",
    "                                      ]), \n",
    "                            lrc_predict(clf, pd.concat([X_oleh_test.apply(lambda x: x.text),\n",
    "                                                        katia_dataset['sentence'], \n",
    "                                                        scraped_dataset['sentence']]),\n",
    "                                         proba=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.68      0.47      0.56       165\n",
      "        tell       0.79      0.90      0.84       366\n",
      "\n",
      "    accuracy                           0.77       531\n",
      "   macro avg       0.74      0.69      0.70       531\n",
      "weighted avg       0.76      0.77      0.75       531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pd.concat([y_oleh_test, \n",
    "                                       katia_dataset['label'], \n",
    "                                       scraped_dataset['label'],\n",
    "                                      ]), \n",
    "                            ensemble_predict(model, clf, pd.concat([X_oleh_test.apply(lambda x: x.text),\n",
    "                                                                    katia_dataset['sentence'], \n",
    "                                                                    scraped_dataset['sentence']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict(\n",
    "    model, \n",
    "    ['Tears are salty.'], proba=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

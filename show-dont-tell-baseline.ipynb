{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Завантаження та підготовка датасетів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tell       431\n",
       "show       179\n",
       "unknown    147\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_train = pd.read_csv('dataset.csv')\n",
    "dataset_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show    39\n",
       "tell    26\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('show-validation.txt', 'r') as f:\n",
    "    show_sents = f.readlines()\n",
    "    \n",
    "with open('tell-validation.txt', 'r') as f:\n",
    "    tell_sents = f.readlines()\n",
    "    \n",
    "dataset_test = pd.DataFrame({'sentence': show_sents + tell_sents,\n",
    "                             'label': ['show'] * len(show_sents) + ['tell'] * len(tell_sents)})\n",
    "\n",
    "dataset_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train['sentence'] = dataset_train['sentence'].apply(nlp)\n",
    "dataset_test['sentence'] = dataset_test['sentence'].apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Роблю з тренувального датасету два датасети: перший буде без речень, проанотованих як 'unknown', а в другому - 'uknown' речення  будуть мати мітку 'show'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tell    431\n",
       "show    179\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_unk = dataset_train[dataset_train['label'] != 'unknown']\n",
    "no_unk['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tell    431\n",
       "show    326\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_as_show = dataset_train.replace({'label': {'unknown': 'show'}})\n",
    "unk_as_show['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перша версія класифікатора. Використовуватиму датасет без 'unknown' речень і просту токенізацію для BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(model):\n",
    "    return [tok.text for tok in model]\n",
    "\n",
    "def lemmatize(model):\n",
    "    return [tok.lemma_ for tok in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.26      0.41        39\n",
      "        tell       0.47      1.00      0.64        26\n",
      "\n",
      "    accuracy                           0.55        65\n",
      "   macro avg       0.74      0.63      0.53        65\n",
      "weighted avg       0.79      0.55      0.50        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def make_classifier():\n",
    "    return Pipeline([('vect', CountVectorizer(lowercase=False, token_pattern=None)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "clf = make_classifier()\n",
    "clf.set_params(vect__tokenizer=tokenize)\n",
    "\n",
    "clf.fit(no_unk['sentence'], no_unk['label'])\n",
    "print(classification_report(dataset_test['label'], clf.predict(dataset_test['sentence'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Підбираю гіперпараметри для класифікатору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.3,\n",
       " 'nb__fit_prior': False,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__tokenizer': <function __main__.tokenize(model)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_grid = [{'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "                   'vect__tokenizer': [tokenize, lemmatize],\n",
    "                   'nb__alpha': [1e-10, 0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 1],\n",
    "                   'nb__fit_prior': [True, False],\n",
    "                  }]\n",
    "\n",
    "gs_clf = GridSearchCV(make_classifier(), parameter_grid, scoring='f1_macro')\n",
    "gs_clf.fit(no_unk['sentence'], no_unk['label'])\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат трохи покращився"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.75      0.38      0.51        39\n",
      "        tell       0.47      0.81      0.59        26\n",
      "\n",
      "    accuracy                           0.55        65\n",
      "   macro avg       0.61      0.60      0.55        65\n",
      "weighted avg       0.64      0.55      0.54        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dataset_test['label'], gs_clf.predict(dataset_test['sentence'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Треную класифікатор на датасеті, де 'unknown'-и промарковані як 'show' і підбираю гіперпараметри для нього."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.82      0.46      0.59        39\n",
      "        tell       0.51      0.85      0.64        26\n",
      "\n",
      "    accuracy                           0.62        65\n",
      "   macro avg       0.66      0.65      0.61        65\n",
      "weighted avg       0.70      0.62      0.61        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier()\n",
    "clf.set_params(vect__tokenizer=tokenize)\n",
    "\n",
    "clf.fit(unk_as_show['sentence'], unk_as_show['label'])\n",
    "print(classification_report(dataset_test['label'], clf.predict(dataset_test['sentence'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.3,\n",
       " 'nb__fit_prior': True,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__tokenizer': <function __main__.lemmatize(model)>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(make_classifier(), parameter_grid, scoring='f1_macro')\n",
    "gs_clf.fit(unk_as_show['sentence'], unk_as_show['label'])\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В порівнянні з попередньою версією, результат покращився"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.81      0.54      0.65        39\n",
      "        tell       0.54      0.81      0.65        26\n",
      "\n",
      "    accuracy                           0.65        65\n",
      "   macro avg       0.67      0.67      0.65        65\n",
      "weighted avg       0.70      0.65      0.65        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dataset_test['label'], gs_clf.predict(dataset_test['sentence'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спостереження і висновки:\n",
    "* розмір валідаційної вибірки досить малий, потрібно буде її розширити:\n",
    "  * планую додати до неї речення, які я вислав Каті на доанотування;\n",
    "  * якщо з доанотовуванням не вийде, то потрібно буде поскрейпити ще даних з сайтів з рекомендаціями для письменників;\n",
    "* у мене була підозра, що при анотуванні, я помилково відносив деякі 'show' речення до 'unknown' категорії. саме тому, я вирішив зробити експерименти з двома датасетами. в експерименті з датасетом, де для 'uknown'-и були промарковані як 'show' якість виявилась кращою. це свідчить про те, що варто додатково переглянути 'unknown' категорію і можливо попереносити деякі речення в 'show'.\n",
    "* BoW + NB показали хороший результат, я очікував гіршого. це водночас плюс (тому що якість хороша :)) і мінус (тому що складніше буде проводити покращення при розробці розумного рішення)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

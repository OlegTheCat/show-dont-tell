{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tell    639\n",
       "show    243\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "oleh_dataset = pd.read_csv('dataset.csv')\n",
    "oleh_dataset = oleh_dataset[oleh_dataset['label'] != 'unknown']\n",
    "oleh_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show    40\n",
       "tell    26\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('show-validation.txt', 'r') as f:\n",
    "    show_sents = f.readlines()\n",
    "    \n",
    "with open('tell-validation.txt', 'r') as f:\n",
    "    tell_sents = f.readlines()\n",
    "    \n",
    "scraped_dataset = pd.DataFrame({'sentence': show_sents + tell_sents,\n",
    "                                'label': ['show'] * len(show_sents) + ['tell'] * len(tell_sents)})\n",
    "\n",
    "scraped_dataset['sentence'] = scraped_dataset['sentence'].str.strip()\n",
    "\n",
    "scraped_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tell    118\n",
       "show     15\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('katia-show.txt', 'r') as f:\n",
    "    show_sents = f.readlines()\n",
    "    \n",
    "with open('katia-tell.txt', 'r') as f:\n",
    "    tell_sents = f.readlines()\n",
    "\n",
    "katia_dataset = pd.DataFrame({'sentence': show_sents + tell_sents,\n",
    "                              'label': ['show'] * len(show_sents) + ['tell'] * len(tell_sents)})\n",
    "\n",
    "katia_dataset['sentence'] = katia_dataset['sentence'].str.strip()\n",
    "\n",
    "katia_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ö–∞—Ç—è –Ω–∞–¥—ñ—Å–ª–∞–ª–∞ —Ä–æ–∑–º—ñ—á–µ–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è, —Ç–æ–º—É –ø–µ—Ä–µ—Ä–∞—Ö—É—é –±–µ–π–∑–ª–∞–π–Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oleh_dataset['sentence'] = oleh_dataset['sentence'].apply(nlp)\n",
    "scraped_dataset['sentence'] = scraped_dataset['sentence'].apply(nlp)\n",
    "katia_dataset['sentence'] = katia_dataset['sentence'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_oleh_train, X_oleh_test, y_oleh_train, y_oleh_test = train_test_split(oleh_dataset['sentence'], oleh_dataset['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def tokenize(model):\n",
    "    return [tok.text for tok in model]\n",
    "\n",
    "def lemmatize(model):\n",
    "    return [tok.lemma_ for tok in model]\n",
    "\n",
    "def make_baseline_clf():\n",
    "    return Pipeline([('vect', CountVectorizer(lowercase=False, token_pattern=None)),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "def validation_report(clf):\n",
    "    print('Oleh:')\n",
    "    print(classification_report(y_oleh_test, clf.predict(X_oleh_test)))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('Scraped:')\n",
    "    print(classification_report(scraped_dataset['label'], clf.predict(scraped_dataset['sentence'])))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('Katia:')\n",
    "    print(classification_report(katia_dataset['label'], clf.predict(katia_dataset['sentence'])))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('All:')\n",
    "    print(classification_report(pd.concat([y_oleh_test, scraped_dataset['label'], katia_dataset['label']]), \n",
    "                                clf.predict(pd.concat([X_oleh_test, scraped_dataset['sentence'], katia_dataset['sentence']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.66      0.34      0.45        56\n",
      "        tell       0.81      0.94      0.87       165\n",
      "\n",
      "    accuracy                           0.79       221\n",
      "   macro avg       0.73      0.64      0.66       221\n",
      "weighted avg       0.77      0.79      0.76       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.83      0.12      0.22        40\n",
      "        tell       0.42      0.96      0.58        26\n",
      "\n",
      "    accuracy                           0.45        66\n",
      "   macro avg       0.62      0.54      0.40        66\n",
      "weighted avg       0.67      0.45      0.36        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.12      0.13      0.13        15\n",
      "        tell       0.89      0.88      0.89       118\n",
      "\n",
      "    accuracy                           0.80       133\n",
      "   macro avg       0.51      0.51      0.51       133\n",
      "weighted avg       0.80      0.80      0.80       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.51      0.23      0.32       111\n",
      "        tell       0.77      0.92      0.84       309\n",
      "\n",
      "    accuracy                           0.74       420\n",
      "   macro avg       0.64      0.58      0.58       420\n",
      "weighted avg       0.70      0.74      0.70       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_baseline_clf()\n",
    "clf.set_params(vect__tokenizer=tokenize)\n",
    "\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.05,\n",
       " 'nb__fit_prior': False,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__tokenizer': <function __main__.tokenize(model)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_grid = [{'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "                   'vect__tokenizer': [tokenize, lemmatize],\n",
    "                   'nb__alpha': [1e-10, 0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 1],\n",
    "                   'nb__fit_prior': [True, False],\n",
    "                  }]\n",
    "\n",
    "gs_clf = GridSearchCV(make_baseline_clf(), parameter_grid, scoring='f1_macro')\n",
    "gs_clf.fit(X_oleh_train, y_oleh_train)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.51      0.62      0.56        56\n",
      "        tell       0.86      0.79      0.83       165\n",
      "\n",
      "    accuracy                           0.75       221\n",
      "   macro avg       0.68      0.71      0.69       221\n",
      "weighted avg       0.77      0.75      0.76       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.71      0.25      0.37        40\n",
      "        tell       0.42      0.85      0.56        26\n",
      "\n",
      "    accuracy                           0.48        66\n",
      "   macro avg       0.57      0.55      0.47        66\n",
      "weighted avg       0.60      0.48      0.45        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.14      0.40      0.20        15\n",
      "        tell       0.90      0.68      0.77       118\n",
      "\n",
      "    accuracy                           0.65       133\n",
      "   macro avg       0.52      0.54      0.49       133\n",
      "weighted avg       0.81      0.65      0.71       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.40      0.46      0.43       111\n",
      "        tell       0.80      0.75      0.77       309\n",
      "\n",
      "    accuracy                           0.68       420\n",
      "   macro avg       0.60      0.61      0.60       420\n",
      "weighted avg       0.69      0.68      0.68       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_report(gs_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def ds_func(f):\n",
    "    return lambda X: [f(x) for x in X]\n",
    "\n",
    "def combine_extractors(funcs):\n",
    "    def combined(x):\n",
    "        feats = {}\n",
    "        for e in funcs:\n",
    "            feats.update(e(x))\n",
    "        return feats\n",
    "    return combined\n",
    "\n",
    "def make_rfc_classifier(*feature_extractors):\n",
    "    classifier = Pipeline([('extractor', FunctionTransformer()),\n",
    "                           ('dict_vect', DictVectorizer()),\n",
    "                           ('rfc', RandomForestClassifier(random_state=42))])\n",
    "    params = {'extractor__func': ds_func(combine_extractors(feature_extractors))}\n",
    "    classifier.set_params(**params)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def make_lrc_classifier(*feature_extractors):\n",
    "    classifier = Pipeline([('extractor', FunctionTransformer()),\n",
    "                           ('dict_vect', DictVectorizer()),\n",
    "                           ('lrc', LogisticRegression())])\n",
    "        \n",
    "    params = {'lrc__random_state': 42,\n",
    "              'lrc__solver': 'sag',\n",
    "              'lrc__multi_class': 'multinomial',\n",
    "              'lrc__max_iter': 5000,\n",
    "              'extractor__func': ds_func(combine_extractors(feature_extractors))}\n",
    "    classifier.set_params(**params)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–∏—Ç—è–≥—É—é —á–∞—Å—Ç–æ—Ç–Ω—ñ—Å—Ç—å POS i DEP —Ç–µ–≥—ñ–≤\n",
    "### –¢—É—Ç –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é Random Forest, –±–æ –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è –≤–∏–¥–∞—î –≤—Å—é–¥–∏ –Ω—É–ª—ñ. –ü–µ–≤–Ω–æ —Ü—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–µ fit-—è—Ç—å—Å—è –ª—ñ–Ω—ñ–π–Ω–æ—é –º–æ–¥–µ–ª–ª—é ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRON_num': 0.16666666666666666,\n",
       " 'VERB_num': 0.16666666666666666,\n",
       " 'NOUN_num': 0.16666666666666666,\n",
       " 'ADV_num': 0.3333333333333333,\n",
       " 'PUNCT_num': 0.16666666666666666}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_pos_freqs(doc):\n",
    "    pos_freqs = Counter([tok.pos_ for tok in doc])    \n",
    "    return {pos + '_num': freq / len(doc) for pos, freq in pos_freqs.items()}\n",
    "\n",
    "extract_pos_freqs(nlp('I like cats very much.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.53      0.34      0.41        56\n",
      "        tell       0.80      0.90      0.85       165\n",
      "\n",
      "    accuracy                           0.76       221\n",
      "   macro avg       0.66      0.62      0.63       221\n",
      "weighted avg       0.73      0.76      0.74       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.15      0.26        40\n",
      "        tell       0.43      1.00      0.60        26\n",
      "\n",
      "    accuracy                           0.48        66\n",
      "   macro avg       0.72      0.57      0.43        66\n",
      "weighted avg       0.78      0.48      0.40        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.13      0.13      0.13        15\n",
      "        tell       0.89      0.89      0.89       118\n",
      "\n",
      "    accuracy                           0.80       133\n",
      "   macro avg       0.51      0.51      0.51       133\n",
      "weighted avg       0.80      0.80      0.80       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.47      0.24      0.32       111\n",
      "        tell       0.77      0.90      0.83       309\n",
      "\n",
      "    accuracy                           0.73       420\n",
      "   macro avg       0.62      0.57      0.58       420\n",
      "weighted avg       0.69      0.73      0.70       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_rfc_classifier(extract_pos_freqs)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dep_freqs(doc):\n",
    "    dep_freqs = Counter([tok.dep_ for tok in doc])\n",
    "    return {dep + '_num': freq / len(doc) for dep, freq in dep_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.78      0.32      0.46        56\n",
      "        tell       0.81      0.97      0.88       165\n",
      "\n",
      "    accuracy                           0.81       221\n",
      "   macro avg       0.80      0.65      0.67       221\n",
      "weighted avg       0.80      0.81      0.77       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.10      0.18        40\n",
      "        tell       0.42      1.00      0.59        26\n",
      "\n",
      "    accuracy                           0.45        66\n",
      "   macro avg       0.71      0.55      0.39        66\n",
      "weighted avg       0.77      0.45      0.34        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.08      0.07      0.07        15\n",
      "        tell       0.88      0.90      0.89       118\n",
      "\n",
      "    accuracy                           0.80       133\n",
      "   macro avg       0.48      0.48      0.48       133\n",
      "weighted avg       0.79      0.80      0.80       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.57      0.21      0.30       111\n",
      "        tell       0.77      0.94      0.85       309\n",
      "\n",
      "    accuracy                           0.75       420\n",
      "   macro avg       0.67      0.58      0.58       420\n",
      "weighted avg       0.72      0.75      0.70       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_rfc_classifier(extract_dep_freqs)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ñ–¥ –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è —Ü–∏—Ö —Ñ—ñ—á —è–∫—ñ—Å—Ç—å –æ—Å–æ–±–ª–∏–≤–æ –Ω–µ –ø–æ–∫—Ä–∞—â—É—î—Ç—å—Å—è :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.68      0.34      0.45        56\n",
      "        tell       0.81      0.95      0.87       165\n",
      "\n",
      "    accuracy                           0.79       221\n",
      "   macro avg       0.74      0.64      0.66       221\n",
      "weighted avg       0.78      0.79      0.77       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.07      0.14        40\n",
      "        tell       0.41      1.00      0.58        26\n",
      "\n",
      "    accuracy                           0.44        66\n",
      "   macro avg       0.71      0.54      0.36        66\n",
      "weighted avg       0.77      0.44      0.31        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.12      0.13      0.13        15\n",
      "        tell       0.89      0.88      0.89       118\n",
      "\n",
      "    accuracy                           0.80       133\n",
      "   macro avg       0.51      0.51      0.51       133\n",
      "weighted avg       0.80      0.80      0.80       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.51      0.22      0.30       111\n",
      "        tell       0.77      0.93      0.84       309\n",
      "\n",
      "    accuracy                           0.74       420\n",
      "   macro avg       0.64      0.57      0.57       420\n",
      "weighted avg       0.70      0.74      0.70       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_rfc_classifier(extract_pos_freqs, extract_dep_freqs)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–±–∏—Ä–∞—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥–æ–ª–æ–≤–Ω—ñ –ø—ñ–¥–º–µ—Ç —ñ –ø—Ä–∏—Å—É–¥–æ–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_verb(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        feats['main-word'] = main.text\n",
    "        feats['main-pos'] = main.pos_\n",
    "        feats['main-lemma'] = main.lemma_\n",
    "        \n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            feats['subj-word'] = subj.text\n",
    "            feats['subj-pos'] = subj.pos_\n",
    "            feats['subj-lemma'] = subj.lemma_\n",
    "            \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –Ø–∫—ñ—Å—Ç—å –Ω–∞ —Ä—ñ–≤–Ω—ñ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.67      0.36      0.47        56\n",
      "        tell       0.81      0.94      0.87       165\n",
      "\n",
      "    accuracy                           0.79       221\n",
      "   macro avg       0.74      0.65      0.67       221\n",
      "weighted avg       0.77      0.79      0.77       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.50      0.03      0.05        40\n",
      "        tell       0.39      0.96      0.56        26\n",
      "\n",
      "    accuracy                           0.39        66\n",
      "   macro avg       0.45      0.49      0.30        66\n",
      "weighted avg       0.46      0.39      0.25        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.12      0.20      0.15        15\n",
      "        tell       0.89      0.82      0.85       118\n",
      "\n",
      "    accuracy                           0.75       133\n",
      "   macro avg       0.51      0.51      0.50       133\n",
      "weighted avg       0.80      0.75      0.78       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.43      0.22      0.29       111\n",
      "        tell       0.76      0.90      0.82       309\n",
      "\n",
      "    accuracy                           0.72       420\n",
      "   macro avg       0.59      0.56      0.56       420\n",
      "weighted avg       0.67      0.72      0.68       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_lrc_classifier(extract_subj_verb)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—ñ–¥–º–µ—Ç—É —ñ –ø—Ä–∏—Å—É–¥–∫–∞. –Ø–∫—ñ—Å—Ç—å –≤ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –∑ RF —Ç—Ä—ñ—à–∫–∏ –ø–æ–∫—Ä–∞—â–∏–ª–∞—Å—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_verb_ctx(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        feats['main-2-word'] = main.nbor(-2).text if main.i > 1 else '<<<none>>>'\n",
    "        feats['main-2-pos'] = main.nbor(-2).pos_ if main.i > 1 else '<<<none>>>'\n",
    "        feats['main-2-lemma'] = main.nbor(-2).lemma_ if main.i > 1 else '<<<none>>>'\n",
    "        feats['main-1-word'] = main.nbor(-1).text if main.i > 0 else '<<<none>>>'\n",
    "        feats['main-1-pos'] = main.nbor(-1).pos_ if main.i > 0 else '<<<none>>>'\n",
    "        feats['main-1-lemma'] = main.nbor(-1).lemma_ if main.i > 0 else '<<<none>>>'\n",
    "        feats['main+1-word'] = main.nbor(1).text if main.i < len(doc) - 1 else '<<<none>>>'\n",
    "        feats['main+1-pos'] = main.nbor(1).pos_ if main.i < len(doc) - 1 else '<<<none>>>'\n",
    "        feats['main+1-lemma'] = main.nbor(1).lemma_ if main.i < len(doc) - 1 else '<<<none>>>'\n",
    "        feats['main+2-word'] = main.nbor(2).text if main.i < len(doc) - 2 else '<<<none>>>'\n",
    "        feats['main+2-pos'] = main.nbor(2).pos_ if main.i < len(doc) - 2 else '<<<none>>>'\n",
    "        feats['main+2-lemma'] = main.nbor(2).lemma_ if main.i < len(doc) - 2 else '<<<none>>>'\n",
    "        \n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            feats['subj-2-word'] = subj.nbor(-2).text if subj.i > 1 else '<<<none>>>'\n",
    "            feats['subj-2-pos'] = subj.nbor(-2).pos_ if subj.i > 1 else '<<<none>>>'\n",
    "            feats['subj-2-lemma'] = subj.nbor(-2).lemma_ if subj.i > 1 else '<<<none>>>'\n",
    "            feats['subj-1-word'] = subj.nbor(-1).text if subj.i > 0 else '<<<none>>>'\n",
    "            feats['subj-1-pos'] = subj.nbor(-1).pos_ if subj.i > 0 else '<<<none>>>'\n",
    "            feats['subj-1-lemma'] = subj.nbor(-1).lemma_ if subj.i > 0 else '<<<none>>>'\n",
    "            feats['subj+1-word'] = subj.nbor(1).text if subj.i < len(doc) - 1 else '<<<none>>>'\n",
    "            feats['subj+1-pos'] = subj.nbor(1).pos_ if subj.i < len(doc) - 1 else '<<<none>>>'\n",
    "            feats['subj+1-lemma'] = subj.nbor(1).lemma_ if subj.i < len(doc) - 1 else '<<<none>>>'\n",
    "            feats['subj+2-word'] = subj.nbor(2).text if subj.i < len(doc) - 2 else '<<<none>>>'\n",
    "            feats['subj+2-pos'] = subj.nbor(2).pos_ if subj.i < len(doc) - 2 else '<<<none>>>'\n",
    "            feats['subj+2-lemma'] = subj.nbor(2).lemma_ if subj.i < len(doc) - 2 else '<<<none>>>'\n",
    "            \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.66      0.45      0.53        56\n",
      "        tell       0.83      0.92      0.87       165\n",
      "\n",
      "    accuracy                           0.80       221\n",
      "   macro avg       0.74      0.68      0.70       221\n",
      "weighted avg       0.79      0.80      0.79       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.12      0.22        40\n",
      "        tell       0.43      1.00      0.60        26\n",
      "\n",
      "    accuracy                           0.47        66\n",
      "   macro avg       0.71      0.56      0.41        66\n",
      "weighted avg       0.77      0.47      0.37        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.16      0.27      0.20        15\n",
      "        tell       0.90      0.82      0.86       118\n",
      "\n",
      "    accuracy                           0.76       133\n",
      "   macro avg       0.53      0.54      0.53       133\n",
      "weighted avg       0.81      0.76      0.78       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.50      0.31      0.38       111\n",
      "        tell       0.78      0.89      0.83       309\n",
      "\n",
      "    accuracy                           0.74       420\n",
      "   macro avg       0.64      0.60      0.61       420\n",
      "weighted avg       0.71      0.74      0.71       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_lrc_classifier(extract_subj_verb, extract_subj_verb_ctx)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –≤ —è–∫–æ—Å—Ç—ñ —Ñ—ñ—á –≤–µ–∫—Ç–æ—Ä —Ä–µ—á–µ–Ω–Ω—è —ñ –≤–µ–∫—Ç–æ—Ä–∏ –≥–æ–ª–æ–≤–Ω–∏—Ö –ø—ñ–¥–º–µ—Ç–∞ —ñ –ø—Ä–∏—Å—É–¥–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_feats(prefix, vector):\n",
    "    feats = {}\n",
    "    \n",
    "    for i, x in enumerate(vector):\n",
    "        feats[prefix + str(i)] = x\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def extract_vector(doc):        \n",
    "    return vector_to_feats('sent_vect', doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.60      0.59      0.59        56\n",
      "        tell       0.86      0.87      0.86       165\n",
      "\n",
      "    accuracy                           0.80       221\n",
      "   macro avg       0.73      0.73      0.73       221\n",
      "weighted avg       0.80      0.80      0.80       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.92      0.28      0.42        40\n",
      "        tell       0.46      0.96      0.62        26\n",
      "\n",
      "    accuracy                           0.55        66\n",
      "   macro avg       0.69      0.62      0.52        66\n",
      "weighted avg       0.74      0.55      0.50        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.26      0.40      0.32        15\n",
      "        tell       0.92      0.86      0.89       118\n",
      "\n",
      "    accuracy                           0.80       133\n",
      "   macro avg       0.59      0.63      0.60       133\n",
      "weighted avg       0.84      0.80      0.82       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.56      0.45      0.50       111\n",
      "        tell       0.82      0.87      0.84       309\n",
      "\n",
      "    accuracy                           0.76       420\n",
      "   macro avg       0.69      0.66      0.67       420\n",
      "weighted avg       0.75      0.76      0.75       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_lrc_classifier(extract_vector)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_main_token(doc):\n",
    "    return [tok for tok in doc if tok.dep_ == 'ROOT'][0]\n",
    "\n",
    "def extract_subj_verb_vector(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        feats.update(vector_to_feats('main_vect', main.vector))\n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            feats.update(vector_to_feats('main_subj_vect', subj.vector))\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.70      0.70      0.70        56\n",
      "        tell       0.90      0.90      0.90       165\n",
      "\n",
      "    accuracy                           0.85       221\n",
      "   macro avg       0.80      0.80      0.80       221\n",
      "weighted avg       0.85      0.85      0.85       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.88      0.17      0.29        40\n",
      "        tell       0.43      0.96      0.60        26\n",
      "\n",
      "    accuracy                           0.48        66\n",
      "   macro avg       0.65      0.57      0.44        66\n",
      "weighted avg       0.70      0.48      0.41        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.16      0.40      0.23        15\n",
      "        tell       0.91      0.73      0.81       118\n",
      "\n",
      "    accuracy                           0.69       133\n",
      "   macro avg       0.53      0.56      0.52       133\n",
      "weighted avg       0.82      0.69      0.74       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.51      0.47      0.49       111\n",
      "        tell       0.81      0.84      0.83       309\n",
      "\n",
      "    accuracy                           0.74       420\n",
      "   macro avg       0.66      0.65      0.66       420\n",
      "weighted avg       0.73      0.74      0.74       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_lrc_classifier(extract_vector, extract_subj_verb_vector)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–∞–∫–µ —Ä—ñ—à–µ–Ω–Ω—è —Ç—Ä—ñ—à–∫–∏ –ø–µ—Ä–µ–≤–µ—Ä—à—É—î –±–µ–π–∑–ª–∞–π–Ω –ø–æ —è–∫–æ—Å—Ç—ñ.\n",
    "### –°–∫–æ–º–±—ñ–Ω—É–≤–∞–≤—à–∏ –≤–µ–∫—Ç–æ—Ä–∏ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ —Ñ—ñ—á–∞–º–∏, —è–∫—ñ—Å—Ç—å —â–µ —Ç—Ä–æ—Ö–∏ –ø–æ–ª—ñ–ø—à—É—î—Ç—å—Å—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.75      0.75      0.75        56\n",
      "        tell       0.92      0.92      0.92       165\n",
      "\n",
      "    accuracy                           0.87       221\n",
      "   macro avg       0.83      0.83      0.83       221\n",
      "weighted avg       0.87      0.87      0.87       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.88      0.17      0.29        40\n",
      "        tell       0.43      0.96      0.60        26\n",
      "\n",
      "    accuracy                           0.48        66\n",
      "   macro avg       0.65      0.57      0.44        66\n",
      "weighted avg       0.70      0.48      0.41        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.13      0.27      0.17        15\n",
      "        tell       0.89      0.77      0.83       118\n",
      "\n",
      "    accuracy                           0.71       133\n",
      "   macro avg       0.51      0.52      0.50       133\n",
      "weighted avg       0.81      0.71      0.75       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.56      0.48      0.51       111\n",
      "        tell       0.82      0.86      0.84       309\n",
      "\n",
      "    accuracy                           0.76       420\n",
      "   macro avg       0.69      0.67      0.68       420\n",
      "weighted avg       0.75      0.76      0.76       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_lrc_classifier(extract_vector, extract_subj_verb_vector, extract_subj_verb, extract_subj_verb_ctx)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–±–∏—Ä–∞—é –Ω-–≥—Ä–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrasefinder import phrasefinder as pf\n",
    "\n",
    "def fetch_ngram(text):\n",
    "#     print('fetching...')\n",
    "    try:\n",
    "        query = pf.escape_query_term(text)\n",
    "        result = pf.search(pf.Corpus.AMERICAN_ENGLISH, query)\n",
    "        if result.error:\n",
    "            print('WARN: request failed: {}'.format(result.error['message']))\n",
    "            return None\n",
    "\n",
    "        return [phrase.match_count for phrase in result.phrases] + [0]\n",
    "    except Exception as error:\n",
    "        print('Fatal error: {}'.format(error))\n",
    "        return None\n",
    "\n",
    "def process_ngram(ngram, res_dict):\n",
    "    def fetch_and_save(text):\n",
    "        if not text in res_dict:\n",
    "            freq = fetch_ngram(text)\n",
    "            if freq is not None:\n",
    "                res_dict[text] = freq\n",
    "    \n",
    "    formatted = ' '.join([x.lower() for x in ngram])\n",
    "    fetch_and_save(formatted)            \n",
    "    return res_dict\n",
    "\n",
    "def collect_ngrams(sents, n, res_dict):\n",
    "    print('starting...')\n",
    "    \n",
    "    for sent in sents:\n",
    "        ngrams = gen_ngrams(sent, n)\n",
    "        if ngrams:\n",
    "            for ngram in ngrams:\n",
    "                process_ngram(ngram, res_dict)\n",
    "    \n",
    "    print('done!')\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ngrams(toks, n):\n",
    "    if len(toks) >= n:\n",
    "        return [toks[i:i+n] for i in range(len(toks) - n + 1)]\n",
    "\n",
    "def get_freqs(toks):\n",
    "    return ngrams[' '.join([x.lower() for x in toks])]\n",
    "\n",
    "def get_or_fetch_freqs(toks):\n",
    "    process_ngram(toks, ngrams)\n",
    "    return get_freqs(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=32)]: Done  49 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=32)]: Done  64 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=32)]: Done  81 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=32)]: Done  98 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=32)]: Done 117 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=32)]: Done 157 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=32)]: Done 178 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=32)]: Done 201 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=32)]: Done 224 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=32)]: Done 249 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=32)]: Done 274 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=32)]: Done 301 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=32)]: Done 328 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=32)]: Done 357 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=32)]: Done 417 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=32)]: Done 448 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=32)]: Done 481 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=32)]: Done 514 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=32)]: Done 549 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=32)]: Done 584 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=32)]: Done 621 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=32)]: Done 658 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=32)]: Done 697 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=32)]: Done 736 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=32)]: Done 800 out of 800 | elapsed: 14.5min finished\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "all_sents = pd.concat([oleh_dataset['sentence'].apply(tokenize), \n",
    "                       scraped_dataset['sentence'].apply(tokenize),\n",
    "                       katia_dataset['sentence'].apply(tokenize)])\n",
    "\n",
    "with open('ngrams.json', 'r') as f:\n",
    "    ngrams = json.load(f)\n",
    "\n",
    "def parallel_collect(sents, ngrams_map):\n",
    "    n_batches = 200\n",
    "    batch_size = math.ceil(len(all_sents) / n_batches)\n",
    "    gen = (delayed(collect_ngrams)(all_sents[i*batch_size:(i+1)*batch_size], n, ngrams)\n",
    "        for i in range(n_batches) for n in range(1, 5))\n",
    "\n",
    "    job_results = Parallel(n_jobs=32, verbose=10)(gen)\n",
    "    \n",
    "    for d in job_results:\n",
    "        ngrams_map.update(d)\n",
    "        \n",
    "    with open('ngrams2.json', 'w') as f:\n",
    "        json.dump(ngrams_map, f)\n",
    "\n",
    "parallel_collect(all_sents, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram_freqs(doc):\n",
    "    feats = {}\n",
    "    \n",
    "    toks = tokenize(doc)\n",
    "    \n",
    "    feats['avg-1-gram-freq'] = np.mean([sum(get_freqs(gram)) for gram in gen_ngrams(toks, 1)])\n",
    "    \n",
    "    if len(doc) >= 2:\n",
    "        feats['avg-2-gram-freq'] = np.mean([sum(get_freqs(gram)) for gram in gen_ngrams(toks, 2)])\n",
    "    if len(doc) >= 3:\n",
    "        feats['avg-3-gram-freq'] = np.mean([sum(get_freqs(gram)) for gram in gen_ngrams(toks, 3)])\n",
    "    if len(doc) >= 4:\n",
    "        feats['avg-4-gram-freq'] = np.mean([sum(get_freqs(gram)) for gram in gen_ngrams(toks, 4)])\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_verb_ngram_freqs(doc):\n",
    "    feats = {}\n",
    "    main = find_main_token(doc)\n",
    "    \n",
    "    if main.pos_ == 'VERB':\n",
    "        subj = None\n",
    "        for tok in doc:\n",
    "            if tok.head.dep_ == 'ROOT' and tok.dep_ == 'nsubj':\n",
    "                subj = tok\n",
    "                break\n",
    "        if subj:\n",
    "            feats['subj-verb-freq'] = sum(get_or_fetch_freqs([subj.text, main.text]))\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ù–∞ —Å–∞–º–∏—Ö –Ω-–≥—Ä–∞–º–∞—Ö —è–∫—ñ—Å—Ç—å –ø–æ–≥–∞–Ω–µ–Ω—å–∫–∞, –∞ LRC –≤–∑–∞–≥–∞–ª—ñ –≤–∏–¥–∞–≤–∞–ª–∞ –Ω—É–ª—ñ, —â–æ –≤ –ø—Ä–∏–Ω—Ü–∏–ø—ñ –æ—á—ñ–∫—É–≤–∞–Ω–æ (–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–µ –ø–æ–º–∞–≥–∞–ª–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.57      0.29      0.38        56\n",
      "        tell       0.79      0.93      0.85       165\n",
      "\n",
      "    accuracy                           0.76       221\n",
      "   macro avg       0.68      0.61      0.62       221\n",
      "weighted avg       0.74      0.76      0.73       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.05      0.10        40\n",
      "        tell       0.41      1.00      0.58        26\n",
      "\n",
      "    accuracy                           0.42        66\n",
      "   macro avg       0.70      0.53      0.34        66\n",
      "weighted avg       0.77      0.42      0.29        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.12      0.13      0.12        15\n",
      "        tell       0.89      0.87      0.88       118\n",
      "\n",
      "    accuracy                           0.79       133\n",
      "   macro avg       0.50      0.50      0.50       133\n",
      "weighted avg       0.80      0.79      0.80       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.43      0.18      0.25       111\n",
      "        tell       0.76      0.91      0.83       309\n",
      "\n",
      "    accuracy                           0.72       420\n",
      "   macro avg       0.59      0.55      0.54       420\n",
      "weighted avg       0.67      0.72      0.68       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_rfc_classifier(extract_ngram_freqs, extract_subj_verb_ngram_freqs)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—ñ—Å–ª—è –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –∑ POS i DEP —á–∞—Å—Ç–æ—Ç–∞–º–∏ –≤—Å–µ –æ–¥–Ω–æ –∑–ª–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oleh:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.62      0.32      0.42        56\n",
      "        tell       0.80      0.93      0.86       165\n",
      "\n",
      "    accuracy                           0.78       221\n",
      "   macro avg       0.71      0.63      0.64       221\n",
      "weighted avg       0.76      0.78      0.75       221\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraped:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       1.00      0.03      0.05        40\n",
      "        tell       0.40      1.00      0.57        26\n",
      "\n",
      "    accuracy                           0.41        66\n",
      "   macro avg       0.70      0.51      0.31        66\n",
      "weighted avg       0.76      0.41      0.25        66\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Katia:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.12      0.13      0.13        15\n",
      "        tell       0.89      0.88      0.89       118\n",
      "\n",
      "    accuracy                           0.80       133\n",
      "   macro avg       0.51      0.51      0.51       133\n",
      "weighted avg       0.80      0.80      0.80       133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        show       0.46      0.19      0.27       111\n",
      "        tell       0.76      0.92      0.83       309\n",
      "\n",
      "    accuracy                           0.73       420\n",
      "   macro avg       0.61      0.55      0.55       420\n",
      "weighted avg       0.68      0.73      0.68       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = make_rfc_classifier(extract_pos_freqs, extract_dep_freqs, extract_ngram_freqs, extract_subj_verb_ngram_freqs)\n",
    "clf.fit(X_oleh_train, y_oleh_train)\n",
    "validation_report(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í–∏—Å–Ω–æ–≤–∫–∏ —ñ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:\n",
    "\n",
    "* —Ñ—ñ—á—ñ, –∑ —è–∫–∏–º–∏ —è —â–µ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞–≤, –∞–ª–µ —è–∫—ñ –Ω–µ –¥–∞–ª–∏ –ø—Ä–∏—Ä–æ—Å—Ç—É –≤ —è–∫–æ—Å—Ç—ñ:\n",
    "  * —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∫–º–µ–Ω—Ç–Ω–∏–∫—ñ–≤;\n",
    "  * –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü—ñ—è –≤–µ–∫—Ç–æ—Ä—ñ–≤ –≥–æ–ª–æ–≤–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω—É —ñ –π–æ–≥–æ –¥—ñ—Ç–µ–π –≤ –¥–µ—Ä–µ–≤—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π;\n",
    "\n",
    "* —Å—Ö–æ–∂–µ –Ω–∞ —Ç–µ, —â–æ –≤ –∫–æ–∂–Ω–æ–≥–æ —Å–≤–æ—î —Ä–æ–∑—É–º—ñ–Ω–Ω—è show i tell —Ä–µ—á–µ–Ω—å :)\n",
    "  * —è —Ä–æ–∑–º—ñ—á–∞–≤ –¥–∞–Ω—ñ –∑–∞ –ø—Ä–∏–Ω—Ü–∏–ø–æ–º, —è–∫—â–æ —î —Ö–æ—á–∞ –± —è–∫–µ—Å—å –º—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑—É–≤–∞–Ω–Ω—è (she felt fear while walking the corridors -> as she walked through the dark corridors, her heartbeat increased with every step), —Ç–æ —Ü–µ —Ä–µ—á–µ–Ω–Ω—è —è –≤–≤–∞–∂–∞–≤ `show`;\n",
    "  * —É –ö–∞—Ç—ñ —Ç–∞ –Ω–∞ —Å–∞–π—Ç–∞—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π —Å—Ö–æ–∂–µ –±—ñ–ª—å—à —Å—Ç—Ä–æ–≥—ñ –≤–∏–º–æ–≥–∏ –¥–æ show;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
